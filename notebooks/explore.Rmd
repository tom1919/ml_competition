---
title: "Untitled"
output: html_document
---
The purpose of this document is explore data and test out some models

File paths, helper functions and packages
```{r}
scripts <- 'C:/Users/tommy/Google Drive/Coursework/1machine_learning/scripts/'
data <- "C:/Users/tommy/Google Drive/Coursework/1machine_learning/data/"
raw <- "C:/Users/tommy/Google Drive/Coursework/1machine_learning/data/raw/"
source(paste0(scripts,'helper_functions.R'))

LoadPackages(c("dplyr", "ggplot2", "caret", "tibble", 'Boruta', 'stringr',
               "scales"))
```

Load data
```{r}
load(paste0(raw,"MLProjectData.RData"))
df <-  MLProjectData %>% dplyr::mutate(id = row_number())
```

Summary of data
```{r}
df_stats <- summarize_df(df)

df_stats %>% arrange(desc(mode_ratio))
```

sum of T/F categorical features
```{r}
cat_log <- df %>% 
  select(starts_with("cat")) %>% 
  select(-cat1, -cat2) %>% # these aren't binary
  mutate(cat_sum = rowSums(.))

# add cat_sum col to df
df$cat_sum <- cat_log$cat_sum 
```

Train test split
```{r}
set.seed(888)
train <- df  %>% sample_frac(.8)
test <- anti_join(df, train, by = "id")
```

histogram of response variable
```{r}
ggplot(data = train, aes(x = target)) +
  geom_histogram(bins = 100, color = "#3F97D0", fill = "#F7AD50")+
  geom_vline(aes(xintercept = mean(train$target)), color = "red") +
  labs(title = "Target Variable Histogram",
       x = "Target Variable",
       y = "Frequency") +
  theme_bw() +
  theme(plot.title = element_text(hjust = .5, size = 20, face = "bold"),
        axis.text=element_text(size=12),
        axis.title=element_text(size=14),
        legend.text=element_text(size=11),
        legend.title=element_text(size=12)) +
  scale_y_continuous(labels = comma) 
```

Control model tuning
```{r}
# this creates an object for controling model tuning in caret's train fucntion
ctrl <- trainControl(method = "repeatedcv", number = 5, repeats = 1,
                     verboseIter = T)
```

Feature importance using lasso regression
```{r}
set.seed(888)

# create tuning grid
lasso_grid <- expand.grid(alpha = 1, # 1 is lasso
                          lambda = c(seq(0.0001, 1, length =  400),
                                     seq(1,5, length = 100))) %>%
  sample_frac(.2) %>% # randomized grid search bc... idk what im doing
  arrange(lambda)

# train a lasso model using cross validation
lasso_fit <- train(target ~ . -id, data = train, tuneGrid = lasso_grid,
                preProcess = c("center", "scale"), metric = "MAE",
                method = "glmnet", trControl = ctrl)

# cross validated MAE on best fit
lasso_fit$results %>% arrange(MAE)

# get the coeficents for best fit lasso model
lasso_coef <- coef(lasso_fit$finalModel, lasso_fit$bestTune$lambda) %>% 
  as.matrix() %>% as.data.frame() %>% rownames_to_column("feat")

names(lasso_coef) <- c("feat", "coef")

# arrange coef by largest absolute value (variable importance)
lasso_coef %>% arrange(desc(abs(coef))) 
```

Feature importance using ridge regression
```{r}
set.seed(888)

# create tuning grid
ridge_grid <- expand.grid(alpha = 0, # 0 is ridge 
                          lambda = c(seq(0.0001, 1, length =  400),
                                     seq(1,5, length = 100))) %>%
  sample_frac(.2) %>% # randomized grid search bc... idk what im doing
  arrange(lambda)

# train ridge model using cross validation
ridge_fit <- train(target ~ . -id, data = train, tuneGrid = ridge_grid,
                preProcess = c("center", "scale"), metric = "MAE",
                method = "glmnet", trControl = ctrl)

# cross validated MAE on best fit
ridge_fit$results %>% arrange(MAE)

# get the coeficents for best fit lridge model
ridge_coef <- coef(ridge_fit$finalModel, ridge_fit$bestTune$lambda) %>% 
  as.matrix() %>% as.data.frame() %>% rownames_to_column("feat")

names(ridge_coef) <- c("feat", "coef")

# arrange coef by largest absolute value (variable importance)
ridge_coef %>% arrange(desc(abs(coef))) 
```

Feature importance using boruta algo
```{r}
set.seed(888)
# run the algorithm
boruta_train <- Boruta(target ~. -id, data = train, doTrace = 2, maxRuns = 20)
# make a rough cut for any variables that the algo wasn't able to decide on
final_boruta <- TentativeRoughFix(boruta_train)

# create a df of feature importance
boruta_df <- attStats(final_boruta) %>% 
  rownames_to_column("col") %>%
  arrange(desc(meanImp))

# look at the variables that were confirmed as important
boruta_df %>% filter(decision == "Confirmed")

# saveRDS(boruta_train, paste0(data, "boruta_train.rds"))
# boruta_train <- readRDS(paste0(data, "boruta_train.rds"))
```

train set: create feeatures
```{r}
# these variables were important based on boruta and ridge
imp_num_vars <- c('num6', 'num5', 'num58', 'num4',
                  'num23', 'num32', "num18", "num58",
                  "num1", "num3", "num5") 

# create square terms
num_vars_df <- train %>% select(imp_num_vars)

sq_num_vars <- num_vars_df * num_vars_df
new_names <- names(sq_num_vars) %>% str_replace_all("num", 'sq_num')
names(sq_num_vars) <- new_names

train <- bind_cols(train, sq_num_vars)


# create interaction terms
int_terms <- do.call(cbind, combn(colnames(num_vars_df), 2, FUN= function(x)
  list(setNames(data.frame(num_vars_df[,x[1]]*num_vars_df[,x[2]]),
                paste(x, collapse="_")) )))

train <- bind_cols(train, int_terms)

# create a new binary feature
train <- train %>% 
  mutate(imp_cat = ifelse(cat13 == T | 
                            cat16 == T |
                            cat6 == T |
                            cat19 == T |
                            cat11 == T, 1, 0 ))
```


Feature importance using elastic net for the newly created data
```{r}
set.seed(888)
# create a tuning grid
enet_grid <- expand.grid(alpha = seq(0,1, length = 15), 
                          lambda = c(seq(0.0001, 1, length =  400),
                                     seq(1,5, length = 100))) %>%
  sample_frac(.015) # # randomized grid search bc... idk what im doing

# train an elastic net model
enet_fit <- train(target ~ . -id, data = train, tuneGrid = enet_grid,
                  preProcess = c("center", "scale"), metric = "MAE",
                  method = "glmnet", trControl = ctrl)

# cross validated MAE on best fit
enet_fit$results %>% arrange(MAE)

# coeficients.... this is getting really repetitive
enet_fit_coef <- coef(enet_fit$finalModel, enet_fit$bestTune$lambda) %>% 
  as.matrix() %>% as.data.frame() %>% rownames_to_column("feat")

names(enet_fit_coef) <- c("feat", "coef")

enet_fit_coef %>% arrange(desc(abs(coef)))
```

Feature importance using boruta algo again using with newly created variables
added
```{r}
set.seed(888)
boruta_train2 <- Boruta(target ~. -id, data = train, doTrace = 2, maxRuns = 20)
final_boruta2 <- TentativeRoughFix(boruta_train2)

boruta_df2 <- attStats(final_boruta2) %>% 
  rownames_to_column("col") %>%
  arrange(desc(meanImp))

# write.csv(boruta_df2, paste0(data, "boruta_df2.csv"))
```

select features for modeling
```{r}
# select the top 40 features that boruta ranked as most important
mod_vars <- boruta_df2 %>%
  slice(1:40) %>% 
  pull(col) # get the col names

mod_vars <- c("id", "target", "cat_sum",mod_vars, "num1", "num4" ) # add a few

train <- train %>% 
  select(mod_vars) 
```


################################################################################
the rest of this is trying some models to see what happens

random forest
```{r}
set.seed(888)
rf_grid <- expand.grid(mtry = c(2,5,9,12), 
                       min.node.size = c( 40, 60, 80, 100),
                       splitrule = c("variance", 'extratrees'))

rf <- train(target ~ ., data = train_std, tuneGrid = rf_grid, num.tree = 500, 
            importance = 'permutation', metric = "MAE", method = "ranger", 
            trControl = ctrl)

rf$results %>% arrange(MAE)
```

xgboost with linear base learner
```{r}
set.seed(888)

xgbl_grid <- expand.grid(eta = 2^seq(-11,-7), 
                         lambda = seq(.01,3, length = 50),
                         alpha = c(0,0.01,0.08,0.1,.5,1),
                         nrounds = c(30,50,75)
                         ) %>%
  sample_frac(.05)

xgbl <- train(target ~ ., data = train2, tuneGrid = xgbl_grid, metric = "MAE",
              method = "xgbLinear", preProcess = c("center", "scale"),
              trControl = ctrl)

xgbl$results %>% arrange(MAE)
```

k nearest neighbor
```{r}
set.seed(888)

knn_grid <- expand.grid(k = seq(40,65,3))
knn <- train(target ~ ., data = train2, tuneGrid = knn_grid ,
             metric = "MAE", method = "knn", trControl = ctrl)
```




